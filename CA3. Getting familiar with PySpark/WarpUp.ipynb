{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Warm-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "* **Alireza Arbabi**\n",
    "* **Hadi Babalou**\n",
    "* **Ali Padyav**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Firing Up Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import mean, stddev, col\n",
    "from pyspark.sql.types import FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML('<style>pre { white-space: pre !important; }</style>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building up the spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = spark.read.csv(\"stocks.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Check the schema of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+---+-----+------+---------+\n",
      "| _c0| _c1| _c2|_c3|  _c4|   _c5|      _c6|\n",
      "+----+----+----+---+-----+------+---------+\n",
      "|Date|Open|High|Low|Close|Volume|Adj Close|\n",
      "+----+----+----+---+-----+------+---------+\n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_frame.show(1)\n",
    "data_frame.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rename the columns of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = data_frame \\\n",
    "    .withColumnRenamed(\"_c0\", \"Date\") \\\n",
    "    .withColumnRenamed(\"_c1\", \"Open\") \\\n",
    "    .withColumnRenamed(\"_c2\", \"High\") \\\n",
    "    .withColumnRenamed(\"_c3\", \"Low\") \\\n",
    "    .withColumnRenamed(\"_c4\", \"Close\") \\\n",
    "    .withColumnRenamed(\"_c5\", \"Volume\") \\\n",
    "    .withColumnRenamed(\"_c6\", \"Adj Close\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our data more clean, let's remove the first row which includes duplicate name of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = data_frame.filter(data_frame['Open'] != 'Open')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: For those records with closing price less than 500, select opening, closing and volume\n",
    " and show them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+---------+\n",
      "|              Open|             Close|   Volume|\n",
      "+------------------+------------------+---------+\n",
      "|        213.429998|        214.009998|123432400|\n",
      "|        214.599998|        214.379993|150476200|\n",
      "|        214.379993|        210.969995|138040000|\n",
      "|            211.75|            210.58|119282800|\n",
      "|        210.299994|211.98000499999998|111902700|\n",
      "|212.79999700000002|210.11000299999998|115557400|\n",
      "|209.18999499999998|        207.720001|148614900|\n",
      "|        207.870005|        210.650002|151473000|\n",
      "|210.11000299999998|            209.43|108223500|\n",
      "|210.92999500000002|            205.93|148516900|\n",
      "+------------------+------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_data = data_frame.filter(data_frame['Close'] < 500).select('Open', 'Close', 'Volume')\n",
    "filtered_data.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Find out records with opening price more than 200 and closing price less than 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+---------+\n",
      "|              Open|             Close|   Volume|\n",
      "+------------------+------------------+---------+\n",
      "|192.36999699999998|        194.729998|187469100|\n",
      "|        195.909998|        195.859997|174585600|\n",
      "|        195.169994|        199.229994|153832000|\n",
      "|        196.730003|        192.050003|189413000|\n",
      "|192.63000300000002|        195.460001|212576700|\n",
      "|        195.690006|194.11999699999998|119567700|\n",
      "|        196.419996|196.19000400000002|158221700|\n",
      "|        195.889997|195.12000700000002| 92590400|\n",
      "|        194.880001|        198.669994|137586400|\n",
      "|        199.999998|        197.059998|143773700|\n",
      "+------------------+------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_data = data_frame.filter((data_frame['Close'] < 200) & (data_frame['Open'] < 200)).select('Open', 'Close', 'Volume')\n",
    "filtered_data.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Extract the year from the date and save it in a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------------------+----------+---------+------------------+\n",
      "|      Date|      Open|      High|               Low|     Close|   Volume|         Adj Close|\n",
      "+----------+----------+----------+------------------+----------+---------+------------------+\n",
      "|2010-01-04|213.429998|214.499996|212.38000099999996|214.009998|123432400|         27.727039|\n",
      "|2010-01-05|214.599998|215.589994|        213.249994|214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06|214.379993|    215.23|        210.750004|210.969995|138040000|27.333178000000004|\n",
      "+----------+----------+----------+------------------+----------+---------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_frame.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------------------+----------+---------+------------------+----+\n",
      "|      Date|      Open|      High|               Low|     Close|   Volume|         Adj Close|Year|\n",
      "+----------+----------+----------+------------------+----------+---------+------------------+----+\n",
      "|2010-01-04|213.429998|214.499996|212.38000099999996|214.009998|123432400|         27.727039|2010|\n",
      "|2010-01-05|214.599998|215.589994|        213.249994|214.379993|150476200|27.774976000000002|2010|\n",
      "|2010-01-06|214.379993|    215.23|        210.750004|210.969995|138040000|27.333178000000004|2010|\n",
      "+----------+----------+----------+------------------+----------+---------+------------------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, split\n",
    "\n",
    "data_frame = data_frame.withColumn(\"Year\", split(col(\"date\"), \"-\")[0])\n",
    "\n",
    "data_frame.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Now, for each year, show the minimum volumes traded, shown in a column named ‘minVolume’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------------------+------------------+------------------+------------------+---------+------------------+---------+\n",
      "|Year|      Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|MinVolume|\n",
      "+----+----------+------------------+------------------+------------------+------------------+---------+------------------+---------+\n",
      "|2010|2010-01-04|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|100901500|\n",
      "|2010|2010-01-05|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|100901500|\n",
      "|2010|2010-01-06|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|100901500|\n",
      "|2010|2010-01-07|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|100901500|\n",
      "|2010|2010-01-08|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|100901500|\n",
      "|2010|2010-01-11|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|100901500|\n",
      "|2010|2010-01-12|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|100901500|\n",
      "|2010|2010-01-13|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|100901500|\n",
      "|2010|2010-01-14|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|100901500|\n",
      "|2010|2010-01-15|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|100901500|\n",
      "|2010|2010-01-19|        208.330002|215.18999900000003|        207.240004|        215.039995|182501900|27.860484999999997|100901500|\n",
      "|2010|2010-01-20|        214.910006|        215.549994|        209.500002|            211.73|153038200|         27.431644|100901500|\n",
      "|2010|2010-01-21|        212.079994|213.30999599999998|        207.210003|        208.069996|152038600|         26.957455|100901500|\n",
      "|2010|2010-01-22|206.78000600000001|        207.499996|            197.16|            197.75|220441900|         25.620401|100901500|\n",
      "|2010|2010-01-25|202.51000200000001|        204.699999|        200.190002|        203.070002|266424900|26.309658000000002|100901500|\n",
      "|2010|2010-01-26|205.95000100000001|        213.710005|        202.580004|        205.940001|466777500|         26.681494|100901500|\n",
      "|2010|2010-01-27|        206.849995|            210.58|        199.530001|        207.880005|430642100|26.932840000000002|100901500|\n",
      "|2010|2010-01-28|        204.930004|        205.500004|        198.699995|        199.289995|293375600|25.819922000000002|100901500|\n",
      "|2010|2010-01-29|        201.079996|        202.199995|        190.250002|        192.060003|311488100|         24.883208|100901500|\n",
      "|2010|2010-02-01|192.36999699999998|             196.0|191.29999899999999|        194.729998|187469100|         25.229131|100901500|\n",
      "+----+----------+------------------+------------------+------------------+------------------+---------+------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_frame.createOrReplaceTempView(\"Stock\")\n",
    "result = spark.sql(\"SELECT Year, Min(Volume) as MinVolume FROM Stock GROUP BY Year\")\n",
    "data_frame = data_frame.join(result, \"Year\", \"inner\")\n",
    "data_frame.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+----------+------------------+----------+---------+------------------+---------+---------+\n",
      "|Year|      Date|      Open|      High|               Low|     Close|   Volume|         Adj Close|MinVolume|YearMonth|\n",
      "+----+----------+----------+----------+------------------+----------+---------+------------------+---------+---------+\n",
      "|2010|2010-01-04|213.429998|214.499996|212.38000099999996|214.009998|123432400|         27.727039|100901500|  2010-01|\n",
      "|2010|2010-01-05|214.599998|215.589994|        213.249994|214.379993|150476200|27.774976000000002|100901500|  2010-01|\n",
      "|2010|2010-01-06|214.379993|    215.23|        210.750004|210.969995|138040000|27.333178000000004|100901500|  2010-01|\n",
      "+----+----------+----------+----------+------------------+----------+---------+------------------+---------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_frame = data_frame.withColumn(\"YearMonth\", col(\"date\").substr(1, 7))\n",
    "data_frame.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+----------+----------+------------------+----------+---------+------------------+---------+---------+\n",
      "|YearMonth|Year|      Date|      Open|      High|               Low|     Close|   Volume|         Adj Close|MinVolume|MaxVolume|\n",
      "+---------+----+----------+----------+----------+------------------+----------+---------+------------------+---------+---------+\n",
      "|  2010-01|2010|2010-01-04|213.429998|214.499996|212.38000099999996|214.009998|123432400|         27.727039|100901500|466777500|\n",
      "|  2010-01|2010|2010-01-05|214.599998|215.589994|        213.249994|214.379993|150476200|27.774976000000002|100901500|466777500|\n",
      "|  2010-01|2010|2010-01-06|214.379993|    215.23|        210.750004|210.969995|138040000|27.333178000000004|100901500|466777500|\n",
      "+---------+----+----------+----------+----------+------------------+----------+---------+------------------+---------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_frame.createOrReplaceTempView(\"Stock\")\n",
    "result = spark.sql(\"SELECT YearMonth, Max(Volume) as MaxVolume FROM Stock GROUP BY YearMonth\")\n",
    "data_frame = data_frame.join(result, \"YearMonth\", \"inner\")\n",
    "data_frame.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8: For the last step, calculate mean and standard deviation of high price over the whole data frame and show them in two decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average of high price is:  315.91\n",
      "The std of high price is:  186.9\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"SELECT Avg(High) as highAvg, STD(High) as highStd FROM Stock\")\n",
    "highAvg = result.collect()[0][0]\n",
    "highSTD = result.collect()[0][1]\n",
    "\n",
    "# Let's round the results in two decimal places\n",
    "\n",
    "print(\"The average of high price is: \", round(highAvg, 2))\n",
    "print(\"The std of high price is: \", round(highSTD, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
