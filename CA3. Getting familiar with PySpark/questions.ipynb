{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Q1.**  Read about how Spark and Hadoop work. What does the term ‘lazy evaluation’ mean for them? Explain with a simple example.\n",
    "\n",
    "**Answer:** Lazy evaluation is an evaluation strategy that delays the evaluation of an expression until its value is actually needed. In Spark, transformations are lazy, which means that they do not compute their results right away. Instead, they just remember the transformations applied to some base data set (e.g. a file). The transformations are only computed when an action requires a result that needs to be returned to the driver program. This is done to optimize the computation and avoid unnecessary calculations. For example, consider the following code snippet:\n",
    "    \n",
    "```python\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "filtered_rdd = rdd.filter(lambda x: x % 2 == 0)\n",
    "mapped_rdd = filtered_rdd.map(lambda x: x * 2)\n",
    "result = mapped_rdd.collect()\n",
    "```\n",
    "\n",
    "In this example, `rdd` is a base RDD created from a list of numbers. The `filter` transformation creates a new RDD `filtered_rdd` by filtering out the odd numbers. The `map` transformation creates another RDD `mapped_rdd` by doubling the values of the even numbers. However, none of these transformations are actually computed until the `collect` action is called. At that point, Spark evaluates the transformations in a lazy manner and computes the final result. This lazy evaluation allows Spark to optimize the computation by chaining together multiple transformations and executing them only when necessary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2**. Your main task’s dataset has about 1,200,000 rows, which makes it quite hard, and even sometimes impossible, to work with. Explain how parquet files try to solve this problem, compared to normal file formats like csv.\n",
    "\n",
    "**Answer:** Parquet files are columnar storage files that store data in a columnar format, which means that the values of a column are stored together. This is in contrast to CSV files, which store data in a row-based format. Parquet files are more efficient than CSV files for several reasons. First, they are highly compressed, which reduces the amount of disk space required to store the data. Second, they are splittable, which means that they can be read in parallel by multiple tasks, making them faster to read and process. Third, they support complex data types, such as nested structures and arrays, which makes them more flexible than CSV files. Finally, they support predicate pushdown, which means that the query engine can push filters down to the storage layer, reducing the amount of data that needs to be read from disk. All of these features make Parquet files a better choice for working with large datasets than CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.** As you might have noticed, Spark doesn’t save checkpoints. How can we enforce it to do so? This can help us if we have multiple computation steps and we don’t want to wait a lot for the result.\n",
    "\n",
    "**Answer:** Spark does not save checkpoints by default, but you can enable checkpointing by calling the `checkpoint()` method on a RDD. Checkpointing is useful when you have multiple computation steps and you want to save the intermediate results to disk so that you can recover from failures without having to recompute the entire computation. To enable checkpointing, you need to set the `CheckpointDir` configuration property to a directory where Spark can save the checkpoint data. For example, you can set the property like this:\n",
    "    \n",
    "```python\n",
    "sc.setCheckpointDir(\"hdfs://path/to/checkpoint_directory\")\n",
    "```\n",
    "which sc is SparkContext.\n",
    "\n",
    "After setting the property, you can call the `checkpoint()` method on a RDD to save the intermediate results to disk. For example:\n",
    "\n",
    "```python\n",
    "data.checkpoint()\n",
    "```\n",
    "\n",
    "This will save the intermediate results of the RDD `data` to the checkpoint directory specified in the `spark.checkpoint.dir` property. You can then recover the intermediate results by reading them from the checkpoint directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.** Top companies stream their data on a regular routine, e.g. daily. How can we save data, so that we could filter it based on specific columns, e.g. date, faster than regular filtering?\n",
    "\n",
    "**Answer:** To save data in a way that allows for faster filtering based on specific columns, you can use partitioning in Spark. Partitioning is the process of dividing the data into multiple partitions based on the values of one or more columns. By partitioning the data, you can organize it in a way that makes it easier and faster to filter based on specific columns. For example, if you have a dataset that contains daily data, you can partition the data by date so that each partition contains the data for a specific date. This allows you to filter the data based on date more efficiently because Spark can skip reading the partitions that do not contain the data you are interested in.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5.** Let's face off Pandas and PySpark in the data analysis arena! When does each library truly shine, and why? Consider factors like data size, processing complexity, and user experience.\n",
    "\n",
    "**Answer:** Pandas and PySpark are both popular libraries for data analysis, but they have different strengths and weaknesses. Pandas is a Python library that is designed for working with small to medium-sized datasets that can fit into memory on a single machine. It provides a powerful and easy-to-use API for data manipulation and analysis, and it is well-suited for interactive data exploration and prototyping. Pandas is great for users who are familiar with Python and want to work with tabular data in a familiar environment.\n",
    "\n",
    "On the other hand, PySpark is a distributed computing framework that is designed for working with large-scale datasets that are too big to fit into memory on a single machine. It provides a scalable and fault-tolerant platform for processing big data using a cluster of machines. PySpark is well-suited for processing complex data transformations and running machine learning algorithms on large datasets. PySpark is great for users who need to work with big data and want to take advantage of distributed computing.\n",
    "\n",
    "In summary, Pandas shines when working with small to medium-sized datasets that can fit into memory on a single machine, while PySpark shines when working with large-scale datasets that require distributed computing. The choice between Pandas and PySpark depends on the size of the data, the complexity of the processing, and the user's familiarity with Python and distributed computing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
